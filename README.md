#  Q1 ViT

Documents a series of systematic experiments conducted on CIFAR-10 to understand the training dynamics and performance behavior of Vision Transformers (ViTs) and their hybrid variants. The objective was not to reach state-of-the-art accuracy, but to explore **how different architectural and training modifications influence model performance** under limited compute.

---

## 1Ô∏è‚É£ Baseline ViT

- Implemented a **custom Vision Transformer** from scratch: patch embedding, positional embeddings, class token, and Transformer encoder blocks with pre-layer normalization and learnable gamma scaling.  
- Trained with standard AdamW and cosine learning rate schedule.  
- **Accuracy:** ~80%  
- **Observation:** Training was stable, but accuracy plateaued early. Smaller patch sizes increased compute cost but didn‚Äôt yield consistent accuracy improvements.

---

## 2Ô∏è‚É£ Data Augmentation & Regularization

- Added **Mixup** and **CutMix** per batch.  
- Used **RandomCrop**, **RandomHorizontalFlip**, **ColorJitter**, and **RandomErasing**.  
- Introduced **DropPath (stochastic depth)** and label smoothing.  
- **Accuracy:** Slight improvement over baseline (~82‚Äì83%).  
- **Observation:** These regularization techniques stabilized training and improved generalization, but the **loss consistently oscillated around 1.2**, indicating a plateau.

---

## 3Ô∏è‚É£ Hybrid CNN + Transformer

- Replaced the patch embedding with a **ResNet-18 backbone** to extract convolutional features.  
- Fed the resulting feature maps into a Transformer encoder (PyTorch implementation).  
- **Accuracy:** ~82%  
- **Observation:** CNN features helped representation, but did not dramatically surpass the ViT baseline. The hybrid architecture slightly improved stability but the loss curve showed similar plateau behavior.

---

## 4Ô∏è‚É£ Explicit Transformer Implementation

- Replaced the built-in Transformer API with **manually coded attention and MLP blocks**, closely following the original ViT paper structure.  
- Incorporated **Pre-Norm**, learnable scaling (Œ≥), and residual connections carefully.  
- **Accuracy:** Similar to hybrid model (~83‚Äì84%), but allowed **more controlled experimentation** with block design.

---

### üß† 5Ô∏è‚É£ Knowledge Distillation

I experimented with **knowledge distillation** using a **ResNet-18 teacher** trained on CIFAR-10 and a **ViT student**. The training used a combination of **KL divergence** and **cross-entropy loss** with temperature scaling.

However, the **student model only achieved 86% accuracy**, whereas the **hybrid CNN+ViT feature mapping approach** reached **87% accuracy**.

**Observation:** In this case, **direct feature extraction from a CNN backbone into the ViT encoder performed better** than distillation. The CNN features provided a stronger inductive bias, leading to improved generalization without extra complexity.
---

## üìä Summary of Results

| Experiment                          | Accuracy |
|--------------------------------------|----------|
| Baseline ViT                         | ~80%     |
| + Regularization & Augmentations     | 82‚Äì83%   |
| Hybrid CNN + Transformer             | ~82%     |
| Explicit Transformer Implementation  | 83‚Äì84%   |
| Knowledge Distillation               | **~86%** |

---

## ‚ú® Key Insights

- **Patch size** affects both compute and accuracy ‚Äî there‚Äôs a sweet spot for small datasets like CIFAR-10.  
- Standard ViT struggles to go beyond ~80% without strong regularization or supervision.  
- Adding a CNN stem provides moderate benefits but doesn‚Äôt fundamentally solve optimization plateaus.  
- Knowledge distillation is the most impactful technique among the ones tried, significantly improving performance with minimal student-side changes.

---

