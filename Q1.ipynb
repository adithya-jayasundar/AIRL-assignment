{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W8CWnMkeXIQt",
        "outputId": "0ce71b3e-f05c-48cc-a036-f8dba00fbbcf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 170M/170M [00:03<00:00, 43.6MB/s]\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 44.7M/44.7M [00:00<00:00, 179MB/s]\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "  warnings.warn(\n",
            "/tmp/ipython-input-638931600.py:118: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=(device.type=='cuda'))\n",
            "Epoch 1/60:   0%|          | 0/782 [00:00<?, ?it/s]/tmp/ipython-input-638931600.py:159: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n",
            "Epoch 1/60: 100%|██████████| 782/782 [01:05<00:00, 12.01it/s]\n",
            "/tmp/ipython-input-638931600.py:101: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 01/60] Loss: 2.1996  Test Acc: 44.26%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/60: 100%|██████████| 782/782 [01:00<00:00, 12.91it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 02/60] Loss: 1.8674  Test Acc: 60.83%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3/60: 100%|██████████| 782/782 [01:01<00:00, 12.74it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 03/60] Loss: 1.7153  Test Acc: 68.56%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4/60: 100%|██████████| 782/782 [00:59<00:00, 13.06it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 04/60] Loss: 1.6391  Test Acc: 72.00%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 5/60: 100%|██████████| 782/782 [00:58<00:00, 13.30it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 05/60] Loss: 1.5864  Test Acc: 75.53%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 6/60: 100%|██████████| 782/782 [00:59<00:00, 13.13it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 06/60] Loss: 1.5466  Test Acc: 76.50%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 7/60: 100%|██████████| 782/782 [01:00<00:00, 13.00it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 07/60] Loss: 1.5252  Test Acc: 77.31%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 8/60: 100%|██████████| 782/782 [00:58<00:00, 13.27it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 08/60] Loss: 1.5173  Test Acc: 77.76%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 9/60: 100%|██████████| 782/782 [00:59<00:00, 13.18it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 09/60] Loss: 1.4949  Test Acc: 76.40%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 10/60: 100%|██████████| 782/782 [01:01<00:00, 12.74it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 10/60] Loss: 1.4968  Test Acc: 78.24%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 11/60: 100%|██████████| 782/782 [01:00<00:00, 12.97it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 11/60] Loss: 1.4733  Test Acc: 78.61%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 12/60: 100%|██████████| 782/782 [01:00<00:00, 12.84it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 12/60] Loss: 1.4544  Test Acc: 79.76%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 13/60: 100%|██████████| 782/782 [01:00<00:00, 12.85it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 13/60] Loss: 1.4407  Test Acc: 80.30%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 14/60: 100%|██████████| 782/782 [01:02<00:00, 12.60it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 14/60] Loss: 1.4189  Test Acc: 80.63%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 15/60: 100%|██████████| 782/782 [01:00<00:00, 12.83it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 15/60] Loss: 1.4221  Test Acc: 81.40%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 16/60: 100%|██████████| 782/782 [01:00<00:00, 12.93it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 16/60] Loss: 1.4212  Test Acc: 81.32%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 17/60: 100%|██████████| 782/782 [01:01<00:00, 12.76it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 17/60] Loss: 1.4207  Test Acc: 82.04%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 18/60: 100%|██████████| 782/782 [00:59<00:00, 13.14it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 18/60] Loss: 1.3878  Test Acc: 81.74%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 19/60: 100%|██████████| 782/782 [01:00<00:00, 13.00it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 19/60] Loss: 1.3620  Test Acc: 83.25%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 20/60: 100%|██████████| 782/782 [00:59<00:00, 13.10it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 20/60] Loss: 1.3561  Test Acc: 82.95%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 21/60: 100%|██████████| 782/782 [01:01<00:00, 12.71it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 21/60] Loss: 1.3750  Test Acc: 82.91%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 22/60: 100%|██████████| 782/782 [01:00<00:00, 12.86it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 22/60] Loss: 1.3541  Test Acc: 83.37%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 23/60: 100%|██████████| 782/782 [01:00<00:00, 12.95it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 23/60] Loss: 1.3558  Test Acc: 83.96%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 24/60: 100%|██████████| 782/782 [00:59<00:00, 13.20it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 24/60] Loss: 1.3542  Test Acc: 83.18%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 25/60: 100%|██████████| 782/782 [01:00<00:00, 13.02it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 25/60] Loss: 1.3500  Test Acc: 82.96%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 26/60: 100%|██████████| 782/782 [00:57<00:00, 13.51it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 26/60] Loss: 1.3519  Test Acc: 83.65%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 27/60: 100%|██████████| 782/782 [00:58<00:00, 13.27it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 27/60] Loss: 1.3375  Test Acc: 83.08%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 28/60: 100%|██████████| 782/782 [01:01<00:00, 12.77it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 28/60] Loss: 1.3585  Test Acc: 83.99%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 29/60: 100%|██████████| 782/782 [01:00<00:00, 13.01it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 29/60] Loss: 1.3317  Test Acc: 83.19%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 30/60: 100%|██████████| 782/782 [00:59<00:00, 13.04it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 30/60] Loss: 1.3341  Test Acc: 83.39%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 31/60: 100%|██████████| 782/782 [00:59<00:00, 13.05it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 31/60] Loss: 1.3095  Test Acc: 81.95%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 32/60: 100%|██████████| 782/782 [01:00<00:00, 13.02it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 32/60] Loss: 1.3173  Test Acc: 84.18%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 33/60: 100%|██████████| 782/782 [00:58<00:00, 13.27it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 33/60] Loss: 1.2949  Test Acc: 84.85%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 34/60: 100%|██████████| 782/782 [00:58<00:00, 13.29it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 34/60] Loss: 1.2712  Test Acc: 85.24%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 35/60: 100%|██████████| 782/782 [00:59<00:00, 13.22it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 35/60] Loss: 1.2925  Test Acc: 85.01%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 36/60: 100%|██████████| 782/782 [01:00<00:00, 12.90it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 36/60] Loss: 1.2944  Test Acc: 85.48%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 37/60: 100%|██████████| 782/782 [00:58<00:00, 13.40it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 37/60] Loss: 1.2880  Test Acc: 85.08%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 38/60: 100%|██████████| 782/782 [00:58<00:00, 13.44it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 38/60] Loss: 1.2801  Test Acc: 84.48%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 39/60: 100%|██████████| 782/782 [00:57<00:00, 13.72it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 39/60] Loss: 1.3098  Test Acc: 85.39%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 40/60: 100%|██████████| 782/782 [00:58<00:00, 13.37it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 40/60] Loss: 1.2729  Test Acc: 85.13%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 41/60: 100%|██████████| 782/782 [00:57<00:00, 13.53it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 41/60] Loss: 1.2748  Test Acc: 85.34%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 42/60: 100%|██████████| 782/782 [00:59<00:00, 13.12it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 42/60] Loss: 1.2527  Test Acc: 86.10%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 43/60: 100%|██████████| 782/782 [00:58<00:00, 13.37it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 43/60] Loss: 1.2622  Test Acc: 85.76%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 44/60: 100%|██████████| 782/782 [00:58<00:00, 13.36it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 44/60] Loss: 1.2848  Test Acc: 86.17%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 45/60: 100%|██████████| 782/782 [00:58<00:00, 13.30it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 45/60] Loss: 1.2792  Test Acc: 85.08%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 46/60: 100%|██████████| 782/782 [00:58<00:00, 13.31it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 46/60] Loss: 1.2628  Test Acc: 86.26%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 47/60: 100%|██████████| 782/782 [01:00<00:00, 13.03it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 47/60] Loss: 1.2638  Test Acc: 85.98%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 48/60: 100%|██████████| 782/782 [00:59<00:00, 13.16it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 48/60] Loss: 1.2557  Test Acc: 85.63%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 49/60: 100%|██████████| 782/782 [00:59<00:00, 13.12it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 49/60] Loss: 1.2382  Test Acc: 86.32%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 50/60: 100%|██████████| 782/782 [00:59<00:00, 13.14it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 50/60] Loss: 1.2310  Test Acc: 86.71%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 51/60: 100%|██████████| 782/782 [01:00<00:00, 12.86it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 51/60] Loss: 1.2668  Test Acc: 86.46%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 52/60: 100%|██████████| 782/782 [00:59<00:00, 13.14it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 52/60] Loss: 1.2556  Test Acc: 86.80%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 53/60: 100%|██████████| 782/782 [00:59<00:00, 13.20it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 53/60] Loss: 1.2371  Test Acc: 86.65%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 54/60: 100%|██████████| 782/782 [00:58<00:00, 13.39it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 54/60] Loss: 1.2178  Test Acc: 86.55%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 55/60: 100%|██████████| 782/782 [01:00<00:00, 12.97it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 55/60] Loss: 1.2069  Test Acc: 86.41%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 56/60: 100%|██████████| 782/782 [00:59<00:00, 13.12it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 56/60] Loss: 1.2209  Test Acc: 87.04%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 57/60: 100%|██████████| 782/782 [00:59<00:00, 13.06it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 57/60] Loss: 1.1948  Test Acc: 86.68%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 58/60: 100%|██████████| 782/782 [00:59<00:00, 13.20it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 58/60] Loss: 1.2120  Test Acc: 86.76%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 59/60: 100%|██████████| 782/782 [01:00<00:00, 13.02it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 59/60] Loss: 1.2137  Test Acc: 86.51%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 60/60: 100%|██████████| 782/782 [00:59<00:00, 13.24it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 60/60] Loss: 1.2217  Test Acc: 86.99%\n",
            "Training done. Best Acc: 87.04%\n"
          ]
        }
      ],
      "source": [
        "#Resnet + ViT model\n",
        "import os\n",
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms, models\n",
        "\n",
        "\n",
        "# Hybrid ResNet-18 + ViT Model\n",
        "\n",
        "class HybridResNetViT(nn.Module):\n",
        "    def __init__(self, num_classes=10, embed_dim=256, depth=4, num_heads=4):\n",
        "        super().__init__()\n",
        "\n",
        "        resnet = models.resnet18(pretrained=True)\n",
        "        self.backbone = nn.Sequential(*list(resnet.children())[:-2])\n",
        "\n",
        "        self.conv_proj = nn.Conv2d(512, embed_dim, kernel_size=1)\n",
        "\n",
        "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
        "        self.pos_embed = None\n",
        "\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=embed_dim, nhead=num_heads, dim_feedforward=embed_dim*4,\n",
        "            dropout=0.1, activation=\"gelu\", batch_first=True, norm_first=True\n",
        "        )\n",
        "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=depth)\n",
        "\n",
        "        self.norm = nn.LayerNorm(embed_dim)\n",
        "        self.fc = nn.Linear(embed_dim, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B = x.size(0)\n",
        "        feats = self.backbone(x)\n",
        "        feats = self.conv_proj(feats)\n",
        "        tokens = feats.flatten(2).transpose(1, 2)\n",
        "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
        "        tokens = torch.cat((cls_tokens, tokens), dim=1)\n",
        "\n",
        "        if self.pos_embed is None or self.pos_embed.size(1) != tokens.size(1):\n",
        "            self.pos_embed = nn.Parameter(torch.zeros(1, tokens.size(1), tokens.size(2), device=x.device))\n",
        "            nn.init.trunc_normal_(self.pos_embed, std=0.02)\n",
        "        tokens = tokens + self.pos_embed\n",
        "\n",
        "        out = self.transformer(tokens)\n",
        "        cls_out = self.norm(out[:, 0])\n",
        "        return self.fc(cls_out)\n",
        "\n",
        "\n",
        "# Dataloaders with Augmentations\n",
        "\n",
        "def get_dataloaders(batch_size=64, img_size=32, num_workers=2):\n",
        "    transform_train = transforms.Compose([\n",
        "        transforms.RandomCrop(img_size, padding=4),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ColorJitter(0.2, 0.2, 0.2, 0.1),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4914,0.4822,0.4465),(0.2470,0.2435,0.2616)),\n",
        "        transforms.RandomErasing(p=0.25)\n",
        "    ])\n",
        "    transform_test = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4914,0.4822,0.4465),(0.2470,0.2435,0.2616))\n",
        "    ])\n",
        "\n",
        "    trainset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
        "    testset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
        "\n",
        "    trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
        "    testloader  = DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
        "    return trainloader, testloader\n",
        "\n",
        "\n",
        "# Training + Evaluation Helpers\n",
        "\n",
        "def rand_bbox(W, H, lam):\n",
        "    cut_rat = math.sqrt(1. - lam)\n",
        "    cut_w = int(W * cut_rat)\n",
        "    cut_h = int(H * cut_rat)\n",
        "    cx = random.randint(0, W - 1)\n",
        "    cy = random.randint(0, H - 1)\n",
        "    bbx1 = max(0, cx - cut_w // 2)\n",
        "    bby1 = max(0, cy - cut_h // 2)\n",
        "    bbx2 = min(W, cx + cut_w // 2)\n",
        "    bby2 = min(H, cy + cut_h // 2)\n",
        "    return bbx1, bby1, bbx2, bby2\n",
        "\n",
        "def evaluate(model, testloader, device):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in testloader:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "            with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n",
        "                outputs = model(images)\n",
        "            _, pred = outputs.max(1)\n",
        "            total += labels.size(0)\n",
        "            correct += pred.eq(labels).sum().item()\n",
        "    return 100.0 * correct / total\n",
        "\n",
        "\n",
        "# Main Training Loop\n",
        "\n",
        "def train(model, trainloader, testloader, device,\n",
        "          epochs=60, lr=3e-4, weight_decay=0.02,\n",
        "          accumulation_steps=2, warmup_epochs=3,\n",
        "          mixup_alpha=0.8, cutmix_prob=0.5, label_smoothing=0.05):\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss(label_smoothing=label_smoothing)\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "    scaler = torch.cuda.amp.GradScaler(enabled=(device.type=='cuda'))\n",
        "\n",
        "    total_steps = len(trainloader) * epochs\n",
        "    warmup_steps = len(trainloader) * warmup_epochs\n",
        "\n",
        "    def lr_lambda(current_step):\n",
        "        if current_step < warmup_steps:\n",
        "            return float(current_step) / float(max(1, warmup_steps))\n",
        "        progress = float(current_step - warmup_steps) / float(max(1, total_steps - warmup_steps))\n",
        "        return 0.5 * (1.0 + math.cos(math.pi * progress))\n",
        "\n",
        "    scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
        "\n",
        "    best_acc = 0.0\n",
        "    os.makedirs(\"checkpoints\", exist_ok=True)\n",
        "    global_step = 0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        optimizer.zero_grad()\n",
        "        for i, (images, labels) in enumerate(tqdm(trainloader, desc=f\"Epoch {epoch+1}/{epochs}\")):\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            # Mixup/CutMix\n",
        "            if mixup_alpha > 0:\n",
        "                lam = np.random.beta(mixup_alpha, mixup_alpha)\n",
        "                index = torch.randperm(images.size(0)).to(device)\n",
        "                if random.random() < cutmix_prob:\n",
        "                    bbx1, bby1, bbx2, bby2 = rand_bbox(images.size(3), images.size(2), lam)\n",
        "                    images[:, :, bby1:bby2, bbx1:bbx2] = images[index, :, bby1:bby2, bbx1:bbx2]\n",
        "                    lam = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (images.size(2) * images.size(3)))\n",
        "                    target_a, target_b = labels, labels[index]\n",
        "                else:\n",
        "                    images = images * lam + images[index] * (1 - lam)\n",
        "                    target_a, target_b = labels, labels[index]\n",
        "            else:\n",
        "                lam = 1.0\n",
        "                target_a, target_b = labels, None\n",
        "\n",
        "            with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n",
        "                outputs = model(images)\n",
        "                if target_b is not None:\n",
        "                    loss = lam * nn.functional.cross_entropy(outputs, target_a, label_smoothing=0.0) + \\\n",
        "                           (1 - lam) * nn.functional.cross_entropy(outputs, target_b, label_smoothing=0.0)\n",
        "                else:\n",
        "                    loss = criterion(outputs, labels)\n",
        "                loss = loss / accumulation_steps\n",
        "\n",
        "            scaler.scale(loss).backward()\n",
        "\n",
        "            if (i + 1) % accumulation_steps == 0 or (i + 1) == len(trainloader):\n",
        "                scaler.step(optimizer)\n",
        "                scaler.update()\n",
        "                optimizer.zero_grad()\n",
        "                scheduler.step()\n",
        "                global_step += 1\n",
        "\n",
        "            running_loss += loss.item() * accumulation_steps\n",
        "\n",
        "        avg_loss = running_loss / len(trainloader)\n",
        "        acc = evaluate(model, testloader, device)\n",
        "        print(f\"[Epoch {epoch+1:02d}/{epochs}] Loss: {avg_loss:.4f}  Test Acc: {acc:.2f}%\")\n",
        "\n",
        "        if acc > best_acc:\n",
        "            best_acc = acc\n",
        "            torch.save(model.state_dict(), \"checkpoints/hybrid_resnet_vit_best.pth\")\n",
        "\n",
        "    torch.save(model.state_dict(), \"checkpoints/hybrid_resnet_vit_final.pth\")\n",
        "    print(f\"Training done. Best Acc: {best_acc:.2f}%\")\n",
        "\n",
        "\n",
        "# Entry Point\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(\"Using device:\", device)\n",
        "\n",
        "    trainloader, testloader = get_dataloaders(batch_size=64, img_size=32, num_workers=2)\n",
        "\n",
        "    model = HybridResNetViT(num_classes=10, embed_dim=256, depth=4, num_heads=4).to(device)\n",
        "\n",
        "    train(model, trainloader, testloader, device,\n",
        "          epochs=60,\n",
        "          lr=3e-4,\n",
        "          weight_decay=0.02,\n",
        "          accumulation_steps=2,\n",
        "          warmup_epochs=5,\n",
        "          mixup_alpha=0.8,\n",
        "          cutmix_prob=0.5,\n",
        "          label_smoothing=0.05)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ViT + Knowledge Distillation CIFAR-10( attained 86% accuracy, but couldn't save model)\n",
        "\n",
        "import os\n",
        "import math\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms, models\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# Utilities\n",
        "\n",
        "def trunc_normal_(tensor, mean=0., std=1.):\n",
        "    with torch.no_grad():\n",
        "        size = tensor.shape\n",
        "        tmp = tensor.new_empty(size + (4,)).normal_()\n",
        "        valid = (tmp < 2) & (tmp > -2)\n",
        "        ind = valid.max(-1, keepdim=True)[1]\n",
        "        tensor.data.copy_(tmp.gather(-1, ind).squeeze(-1))\n",
        "        tensor.data.mul_(std).add_(mean)\n",
        "        return tensor\n",
        "\n",
        "def rand_bbox(W, H, lam):\n",
        "    cut_rat = math.sqrt(1. - lam)\n",
        "    cut_w = int(W * cut_rat)\n",
        "    cut_h = int(H * cut_rat)\n",
        "    cx = random.randint(0, W - 1)\n",
        "    cy = random.randint(0, H - 1)\n",
        "    bbx1 = max(0, cx - cut_w // 2)\n",
        "    bby1 = max(0, cy - cut_h // 2)\n",
        "    bbx2 = min(W, cx + cut_w // 2)\n",
        "    bby2 = min(H, cy + cut_h // 2)\n",
        "    return bbx1, bby1, bbx2, bby2\n",
        "\n",
        "\n",
        "# DropPath\n",
        "\n",
        "class DropPath(nn.Module):\n",
        "    def __init__(self, drop_prob: float = 0.):\n",
        "        super().__init__()\n",
        "        self.drop_prob = drop_prob\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        if self.drop_prob == 0. or not self.training:\n",
        "            return x\n",
        "        keep_prob = 1 - self.drop_prob\n",
        "        shape = (x.shape[0],) + (1,) * (x.ndim - 1)\n",
        "        rand = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n",
        "        rand.floor_()\n",
        "        return x.div(keep_prob) * rand\n",
        "\n",
        "\n",
        "# Patch Embedding\n",
        "\n",
        "class PatchEmbed(nn.Module):\n",
        "    def __init__(self, img_size=32, patch_size=4, in_channels=3, embed_dim=224):\n",
        "        super().__init__()\n",
        "        assert img_size % patch_size == 0\n",
        "        self.img_size = img_size\n",
        "        self.patch_size = patch_size\n",
        "        self.n_patches = (img_size // patch_size) ** 2\n",
        "        k = patch_size + 2\n",
        "        self.proj = nn.Conv2d(in_channels, embed_dim, kernel_size=k, stride=patch_size, padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.proj(x).flatten(2).transpose(1,2)\n",
        "        return x\n",
        "\n",
        "\n",
        "# Attention, MLP, TransformerBlock\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, dim, num_heads=7, qkv_bias=True, dropout=0.):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        head_dim = dim // num_heads\n",
        "        assert dim % num_heads == 0\n",
        "        self.scale = head_dim ** -0.5\n",
        "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
        "        self.attn_drop = nn.Dropout(dropout)\n",
        "        self.proj = nn.Linear(dim, dim)\n",
        "        self.proj_drop = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, N, D = x.shape\n",
        "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, D // self.num_heads).permute(2,0,3,1,4)\n",
        "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
        "        attn = (q @ k.transpose(-2,-1)) * self.scale\n",
        "        attn = attn.softmax(dim=-1)\n",
        "        attn = self.attn_drop(attn)\n",
        "        x = (attn @ v).transpose(1,2).reshape(B,N,D)\n",
        "        x = self.proj(x)\n",
        "        x = self.proj_drop(x)\n",
        "        return x\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, in_features, hidden_features, out_features, dropout=0.):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
        "        self.act = nn.GELU()\n",
        "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "    def forward(self,x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.act(x)\n",
        "        x = self.drop(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.drop(x)\n",
        "        return x\n",
        "\n",
        "class TransformerEncoderBlock(nn.Module):\n",
        "    def __init__(self, dim, num_heads, mlp_ratio=3.0, qkv_bias=True, dropout=0., drop_path=0.):\n",
        "        super().__init__()\n",
        "        self.norm1 = nn.LayerNorm(dim, eps=1e-6)\n",
        "        self.attn = Attention(dim, num_heads=num_heads, qkv_bias=qkv_bias, dropout=dropout)\n",
        "        self.drop_path1 = DropPath(drop_path) if drop_path > 0 else nn.Identity()\n",
        "        self.norm2 = nn.LayerNorm(dim, eps=1e-6)\n",
        "        mlp_hidden = int(dim * mlp_ratio)\n",
        "        self.mlp = MLP(dim, mlp_hidden, dim, dropout=dropout)\n",
        "        self.drop_path2 = DropPath(drop_path) if drop_path > 0 else nn.Identity()\n",
        "        self.gamma1 = nn.Parameter(1e-4 * torch.ones(dim))\n",
        "        self.gamma2 = nn.Parameter(1e-4 * torch.ones(dim))\n",
        "    def forward(self,x):\n",
        "        x = x + self.drop_path1(self.gamma1 * self.attn(self.norm1(x)))\n",
        "        x = x + self.drop_path2(self.gamma2 * self.mlp(self.norm2(x)))\n",
        "        return x\n",
        "\n",
        "# Vision Transformer\n",
        "\n",
        "class VisionTransformer(nn.Module):\n",
        "    def __init__(self,\n",
        "                 img_size=32,\n",
        "                 patch_size=4,\n",
        "                 in_channels=3,\n",
        "                 num_classes=10,\n",
        "                 embed_dim=224,\n",
        "                 depth=7,\n",
        "                 num_heads=7,\n",
        "                 mlp_ratio=3.0,\n",
        "                 dropout=0.0,\n",
        "                 drop_path_rate=0.05):\n",
        "        super().__init__()\n",
        "        self.patch_embed = PatchEmbed(img_size=img_size, patch_size=patch_size,\n",
        "                                      in_channels=in_channels, embed_dim=embed_dim)\n",
        "        num_patches = self.patch_embed.n_patches\n",
        "\n",
        "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
        "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim))\n",
        "        self.pos_drop = nn.Dropout(dropout)\n",
        "\n",
        "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]\n",
        "        self.blocks = nn.ModuleList([\n",
        "            TransformerEncoderBlock(embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio,\n",
        "                                    qkv_bias=True, dropout=dropout, drop_path=dpr[i])\n",
        "            for i in range(depth)\n",
        "        ])\n",
        "\n",
        "        self.norm = nn.LayerNorm(embed_dim, eps=1e-6)\n",
        "        self.pre_logits = nn.Sequential(nn.LayerNorm(embed_dim), nn.Linear(embed_dim, embed_dim))\n",
        "        self.head = nn.Linear(embed_dim, num_classes)\n",
        "\n",
        "        trunc_normal_(self.pos_embed, std=0.02)\n",
        "        trunc_normal_(self.cls_token, std=0.02)\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, m):\n",
        "        if isinstance(m, nn.Linear):\n",
        "            nn.init.xavier_uniform_(m.weight)\n",
        "            if m.bias is not None: nn.init.zeros_(m.bias)\n",
        "        elif isinstance(m, nn.Conv2d):\n",
        "            nn.init.kaiming_normal_(m.weight, mode='fan_out')\n",
        "            if m.bias is not None: nn.init.zeros_(m.bias)\n",
        "        elif isinstance(m, nn.LayerNorm):\n",
        "            nn.init.ones_(m.weight)\n",
        "            nn.init.zeros_(m.bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B = x.shape[0]\n",
        "        x = self.patch_embed(x)\n",
        "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
        "        x = torch.cat((cls_tokens, x), dim=1)\n",
        "        x = x + self.pos_embed\n",
        "        x = self.pos_drop(x)\n",
        "        for blk in self.blocks: x = blk(x)\n",
        "        x = self.norm(x)\n",
        "        cls_out = x[:, 0]\n",
        "        x = self.pre_logits(cls_out)\n",
        "        x = self.head(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# DataLoaders + Augmentations\n",
        "\n",
        "def get_dataloaders(batch_size=16, img_size=32, num_workers=2):\n",
        "    transform_train = transforms.Compose([\n",
        "        transforms.RandomCrop(img_size, padding=4),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ColorJitter(0.2,0.2,0.2,0.1),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4914,0.4822,0.4465),(0.2470,0.2435,0.2616)),\n",
        "        transforms.RandomErasing(p=0.25)\n",
        "    ])\n",
        "    transform_test = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4914,0.4822,0.4465),(0.2470,0.2435,0.2616))\n",
        "    ])\n",
        "\n",
        "    trainset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
        "    testset  = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
        "    trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
        "    testloader  = DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
        "    return trainloader, testloader\n",
        "\n",
        "\n",
        "# Knowledge Distillation Loss\n",
        "\n",
        "class DistillationLoss(nn.Module):\n",
        "    def __init__(self, alpha=0.5, temperature=4.0):\n",
        "        super().__init__()\n",
        "        self.alpha = alpha\n",
        "        self.temperature = temperature\n",
        "        self.ce = nn.CrossEntropyLoss()\n",
        "        self.kl = nn.KLDivLoss(reduction='batchmean')\n",
        "\n",
        "    def forward(self, student_logits, labels, teacher_logits):\n",
        "        # CE with ground truth\n",
        "        loss_ce = self.ce(student_logits, labels)\n",
        "        # KL with teacher\n",
        "        T = self.temperature\n",
        "        student_soft = nn.functional.log_softmax(student_logits / T, dim=1)\n",
        "        teacher_soft = nn.functional.softmax(teacher_logits / T, dim=1)\n",
        "        loss_kl = self.kl(student_soft, teacher_soft) * (T * T)\n",
        "        # combined\n",
        "        return (1 - self.alpha) * loss_ce + self.alpha * loss_kl\n",
        "\n",
        "\n",
        "# Evaluation\n",
        "\n",
        "def evaluate(model, testloader, device):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in testloader:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, pred = outputs.max(1)\n",
        "            total += labels.size(0)\n",
        "            correct += pred.eq(labels).sum().item()\n",
        "    return 100.0 * correct / total\n",
        "\n",
        "\n",
        "# Training Loop with Distillation\n",
        "def train_distill(student, teacher, trainloader, testloader, device,\n",
        "                   epochs=90, lr=3e-4, weight_decay=0.02, accumulation_steps=2,\n",
        "                   warmup_epochs=5, mixup_alpha=0.8, cutmix_prob=0.5):\n",
        "\n",
        "    criterion = DistillationLoss(alpha=0.5, temperature=4.0)\n",
        "    optimizer = optim.AdamW(student.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "    scaler = torch.cuda.amp.GradScaler(enabled=(device.type=='cuda'))\n",
        "    total_steps = len(trainloader) * epochs\n",
        "    warmup_steps = len(trainloader) * warmup_epochs\n",
        "    def lr_lambda(current_step):\n",
        "        if current_step < warmup_steps:\n",
        "            return float(current_step) / max(1, warmup_steps)\n",
        "        progress = float(current_step - warmup_steps) / float(max(1, total_steps - warmup_steps))\n",
        "        return 0.5 * (1.0 + math.cos(math.pi * progress))\n",
        "    scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
        "    best_acc = 0.0\n",
        "    os.makedirs(\"checkpoints\", exist_ok=True)\n",
        "    global_step = 0\n",
        "\n",
        "    teacher.eval()  # teacher is frozen\n",
        "    for epoch in range(epochs):\n",
        "        student.train()\n",
        "        running_loss = 0.0\n",
        "        optimizer.zero_grad()\n",
        "        for i, (images, labels) in enumerate(tqdm(trainloader, desc=f\"Epoch {epoch+1}/{epochs}\")):\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            # Mixup / CutMix\n",
        "            apply_aug = (mixup_alpha > 0)\n",
        "            if apply_aug:\n",
        "                if random.random() < cutmix_prob:\n",
        "                    lam = np.random.beta(mixup_alpha, mixup_alpha)\n",
        "                    index = torch.randperm(images.size(0)).to(images.device)\n",
        "                    bbx1, bby1, bbx2, bby2 = rand_bbox(images.size(3), images.size(2), lam)\n",
        "                    images[:, :, bby1:bby2, bbx1:bbx2] = images[index, :, bby1:bby2, bbx1:bbx2]\n",
        "                    lam_adj = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (images.size(2) * images.size(3)))\n",
        "                    target_a, target_b = labels, labels[index]\n",
        "                else:\n",
        "                    lam = np.random.beta(mixup_alpha, mixup_alpha)\n",
        "                    index = torch.randperm(images.size(0)).to(images.device)\n",
        "                    images = images * lam + images[index] * (1 - lam)\n",
        "                    target_a, target_b = labels, labels[index]\n",
        "                    lam_adj = lam\n",
        "            else:\n",
        "                target_a, target_b = labels, None\n",
        "                lam_adj = 1.0\n",
        "\n",
        "            with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n",
        "                student_logits = student(images)\n",
        "                with torch.no_grad():\n",
        "                    teacher_logits = teacher(images)\n",
        "                if target_b is not None:\n",
        "                    # weighted CE for mixup\n",
        "                    loss = lam_adj * criterion(student_logits, target_a, teacher_logits) + \\\n",
        "                           (1 - lam_adj) * criterion(student_logits, target_b, teacher_logits)\n",
        "                else:\n",
        "                    loss = criterion(student_logits, labels, teacher_logits)\n",
        "                loss = loss / accumulation_steps\n",
        "\n",
        "            scaler.scale(loss).backward()\n",
        "\n",
        "            if (i + 1) % accumulation_steps == 0 or (i + 1) == len(trainloader):\n",
        "                scaler.step(optimizer)\n",
        "                scaler.update()\n",
        "                optimizer.zero_grad()\n",
        "                scheduler.step()\n",
        "                global_step += 1\n",
        "\n",
        "            running_loss += loss.item() * accumulation_steps\n",
        "\n",
        "        avg_loss = running_loss / len(trainloader)\n",
        "        acc = evaluate(student, testloader, device)\n",
        "        print(f\"[Epoch {epoch+1}/{epochs}] Loss: {avg_loss:.4f}  Test Acc: {acc:.2f}%\")\n",
        "\n",
        "        if acc > best_acc:\n",
        "            best_acc = acc\n",
        "            torch.save(student.state_dict(), \"checkpoints/vit_student_best.pth\")\n",
        "\n",
        "    torch.save(student.state_dict(), \"checkpoints/vit_student_final.pth\")\n",
        "    print(f\"Training done. Best Acc: {best_acc:.2f}%. Final model saved.\")\n",
        "\n",
        "\n",
        "# Main\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(\"Using device:\", device)\n",
        "\n",
        "    BATCH_SIZE = 16\n",
        "    ACCUM_STEPS = 2\n",
        "    EPOCHS = 90\n",
        "    WARMUP = 5\n",
        "\n",
        "    trainloader, testloader = get_dataloaders(batch_size=BATCH_SIZE, img_size=32, num_workers=2)\n",
        "\n",
        "    # Teacher: ResNet-18 pretrained on CIFAR-10 (or train quickly first)\n",
        "    teacher = models.resnet18(num_classes=10)\n",
        "    teacher = teacher.to(device)\n",
        "    # You can load a pretrained checkpoint here if available\n",
        "    # teacher.load_state_dict(torch.load(\"resnet18_cifar10.pth\"))\n",
        "\n",
        "    # Student: ViT\n",
        "    student = VisionTransformer(\n",
        "        img_size=32,\n",
        "        patch_size=4,\n",
        "        embed_dim=224,\n",
        "        depth=7,\n",
        "        num_heads=7,\n",
        "        mlp_ratio=3.0,\n",
        "        dropout=0.1,\n",
        "        drop_path_rate=0.05\n",
        "    ).to(device)\n",
        "\n",
        "    train_distill(student, teacher, trainloader, testloader, device,\n",
        "                   epochs=EPOCHS,\n",
        "                   lr=3e-4,\n",
        "                   weight_decay=0.02,\n",
        "                   accumulation_steps=ACCUM_STEPS,\n",
        "                   warmup_epochs=WARMUP,\n",
        "                   mixup_alpha=0.8,\n",
        "                   cutmix_prob=0.5)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
